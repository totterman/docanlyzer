{
  "title": "Multilingual investor assistant",
  "error": "An error has occurred:",
  "home2": "Moreover, the frontend is multilingual. Demo options are English, Finnish and Swedish. Signing in can be tried with credentials:",
  "home3": "The frontend is implemented in React and Typescript, and decorated with Tailwind CSS. React Hooks useContext, useEffect, useLoaderData and useReducer are put in use. Repeating HTML code is avoided with the style-functions Pagestyle, Warningstyle, and SectionStyle.",
  "explain": "Explain",
  "instructions": "Instructions",
  "instruction1": "Ask the Assistant about 2023 annual report information from companies:",
  "instruction2": "Aker, DnB, Huhtam√§ki, Nokia, Sandvik, Sanoma, Scania, Skanska, Telenor, UPM, Volvo, Yara.",
  "instruction3": "The Assistant currently understands only English.",
  "instruction4": "A good question is:",
  "instruction5": "\"What are the main strategic goals of Sanoma?\"",
  "instruction6": "DISCLAIMER: This app is only for demonstration purposes. DO NOT trust the answers in any way!",
  "question": "Question:",
  "send": "Send",
  "clear": "Clear",
  "ex1q": "What is the purpose of this web application?",
  "ex1a": "To show how non-public information can be leveraged within an AI application. The process is known as retrieval augmented generation, whereby non-public documents are incorporated into the AI language model's knowledge base.",
  "ex2q": "What AI/LLM/model/whatever did you use?",
  "ex2a": "The original development was made with Ollama 7B and Mistral running locally from a Docker container image. However, this application is hosted on Azure, where running huge language model containers comes with a cost. Thus, I decided to run this application with Azure OpenAI, using the GPT 3.5 Turbo 4k model.",
  "ex3q": "Did you just say that an AI language model can be run... on our local office computers?",
  "ex3a": "Exactly! Good (= expensive) hardware, of course, enhances the user experience. However, the underlying concept is that you can host your AI infrastructure entirely within your secure, closed network. Therefore, there is no need to transmit sensitive data over the internet.",
  "ex4q": "And the rest of your technology stack? Do we need more data scientists?",
  "ex4a": "They could certainly improve the quality of the answers! However, this particular application is constructed using solely standard enterprise Java components. It consists of a React client and a Spring Boot server, with no additional complexities. Therefore, it should be relatively straightforward to integrate with your existing enterprise solutions, particularly if you are already utilizing Java/Spring.",
  "ex5q": "Why annual reports?",
  "ex5a": "For a demonstration, there must necessarily be some novel data that cannot realistically be obtained from an existing language model. The demo data is quite recent, and sourced directly from the actual companies' websites.",
  "ex6q": "Do you have any connection to the companies in question?",
  "ex6a": "No. Not in any way. They were selected only for being quite well known, and having an easily accessible annual report available.",
  "ex7q": "I find the Assistant often provides misleading, sometimes plainly wrong information. Why is that?",
  "ex7a": "In annual reports, the most important information is frequently placed in tables. However, tables in PDF files can be structured in many different ways. This is very challenging for the software parsing the files, and the main source of errors in the answers.",
  "ex8q": "Any hopes for improvement?",
  "ex8a": "I think there is. Libraries are advancing rapidly, and new methods for document processing become available.",
  "ex9q": "Are you planning to develop this project further?",
  "ex9a": "Yes I am. For instance, I assume better precision could be gained using the graph database more efficiently. With AI assistance, company information in whichever format it comes, could be classified according to a domain specific taxonomy, implemented by the graph database.",
  "ex10q": "Excellent! How can we reach you?",
  "ex10a1": "Drop a mail to ",
  "ex10a2": " and expect a prompt answer. That said, as a sole proprietor, I sometimes find it impractical to be constantly online.",
  "ttHomepage": "To homepage",
  "ttClear": "Clear chat history",
  "schema": "Schema",
  "scA": "System setup",
  "scB": "You are using the production instance of the app. It is running in a container on Azure Spring Apps. Azure OpenAI is used as the LLM, and Neo4j AURA DB as the graph database. Both of these are cloud based services to avoid the costs from running more Azure containers.",
  "scC": "Development runs on a Linux workstation with 32GB RAM and a GPU. A Spring Boot app reads PDF files and sends their contents to nlp-ingestor for parsing into JSON chunks, which the Spring Boot app organises in a tree structure. In development, both the neo4j database and the Ollama LLM run in Docker containers.",
  "sc1": "A collection of PDF documents is processed by the Spring Boot app and sent to nlp-ingestor.",
  "sc2": "nlp-ingestor runs in a Docker container. It receives a PDF file and parses the contents into JSON blocks. The JSON blocks are tagged as headers, tables, table rows etc.",
  "sc3": "The Spring Boot app receives the JSON blocks and builds a tree structure from them, mirroring the original PDF document structure.",
  "sc4": "The Spring Boot app loads the tree structured document blocks into the neo4j database. Block types are mapped as entities, and their relations follow the tree structure.",
  "sc5": "Embeddings from the document text elements are created using a LLM embedding model. Embeddings are stored in the neo4j database. Relations from the embeddings to their origin text block are also stored in neo4j.",
  "sc6": "Users (= you) can make questions to the system through this React user interface. The question is fed to the AI system, which seeks the most similar text chunks from the neo4j database.",
  "sc7": "Using the most similar text pieces, the LLM formulates an answer to the question and returns it to the user.",
  "scD": "Graph database element structure",
  "scE": "The structure of the neo4j database. A Document consists of Sections, which may be text Chunks or Tables. In other words, Sections belong to Documents, or (subsections) to higher level Sections. Text Chunks and Tables belong in Sections, and have Embeddings constructed from their textual content.",
  "scF": "Acknowledgements",
  "scG": "Thanks to Joshua Yu and Ambit Shukla for their foundational work!",
  "scH": ""
}
